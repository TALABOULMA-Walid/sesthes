\chapter{\label{ch:5-sched}Multi-core Scheduling of FMU Dependence Graphs} 

\minitoc

This chapter presents methods for scheduling an operation graph on a multi-core architecture. Once the operation graph has been constructed and undergone the different phases of transformations as shown in the previous chapter, it is scheduled on the multi-core platform. First, we consider scheduling the operation with the goal of accelerating the execution of co-simulation. Second, we consider scheduling the operation graph while satisfying real-time constraints.  

\section{Scheduling of Dependence Graphs for Co-simulation Acceleration}

In order to achieve fast execution of the co-simulation on a multi-core processor, an efficient allocation and scheduling of the operation graph has to be achieved. The scheduling algorithm takes into account functional and non functional specification in order to produce an allocation of the operation graph vertices (operations) to the cores of the processor, and assign a starting time to each operation. We present here-after a linear programming model and a heuristic for scheduling FMU dependence graphs on multi-core processors with the aim of accelerating the execution of the co-simulation.

\subsection{\label{5:shed-prob}Problem Formulation}

The acceleration of the co-simulation corresponds to the minimization of the makespan of the dependence graph. The makespan is the total execution time of the whole graph. The dependence graph that is fed as input to the scheduling algorithm is a DAG, therefore, it represents a partial order relationship in the execution of the operations, since two operations connected by an arc must be executed sequentially whereas the other ones can be executed in parallel. The scheduling algorithm makes decisions on allocating the operations to the cores while respecting this partial order and trying to minimize the total execution time of the dependence graph. In addition to the execution time of the operations, the scheduling algorithm has to take into considerations, the cost of inter-core synchronization. The scheduling problem can be stated as an optimization problem as follows:

\begin{table}[h]
\centering
\begin{tabular}{l  l}
  \rule{0pt}{5ex}	
	\textit{Input} & Operation graph $G_F(V_F,A_F)$\\
	\rule{0pt}{5ex}									  
	
  \textit{Output} & Offline Schedule of operations on multi-core processor\\
	\rule{0pt}{5ex}									  
  
	\textit{Find} & Allocation of operations to cores, $\alpha: V \rightarrow P$\\
	\rule{0pt}{5ex}
                & Assignment of start times to operations, $\beta: V \times P \rightarrow \mathbb{N}$\\
	\rule{0pt}{5ex}
	
	\textit{Minimize} & Makespan of the graph $P = max(E(o_i))_{o_i \in V}$\\
	\rule{0pt}{5ex}									 
	
	\textit{Subject to} & Precedence constraints of the graph $G_F(V_F,A_F)$\\
										 
	
\end{tabular}
\end{table}

\subsection{Resolution using Linear Programming}

In this section, we give our ILP formulation of the task scheduling problem for the acceleration of FMU co-simulation.

%\subsubsection{Variables and constants}
%
%Tables \ref{tab:varilpsched} and \ref{tab:consilpsched} summarize the variables and the constants that are used in the ILP formulation of the task scheduling problem for the acceleration of FMU co-simulation.
%
%\begin{table}
%\caption{Variables used in the ILP formulation of the task scheduling problem for the acceleration of FMU co-simulation}
%\centering
%\label{tab:varilpsched}
%\begin{tabular}{c c}
%\toprule
%Variable & Type \\
%\midrule
 %$x(o_i)$ & Binary  \\
%$S(o_i)$ & Integer   \\
%$E(o_i)$ & Integer  \\
%$sync_{ijp}$ & Binary   \\
%$b_{ijp}$ & Binary  \\
%$Q_{ip}$ & Integer  \\
%$V_{ip}$ & Binary  \\
%$mkp$ & Integer  \\
%
%\bottomrule
%\end{tabular}
%\end{table}
%
%\begin{table}
%\caption{Constants used in the ILP formulation of the task scheduling problem for the acceleration of FMU co-simulation}
%\centering
%\label{tab:consilpsched}
%\begin{tabular}{c c}
%\toprule
%Constant & Type \\
%\midrule
%
%$M$ & Integer  \\
%$C(o_i)$ & Integer  \\
%
%\bottomrule
%\end{tabular}
%\end{table}

\subsubsection{Constraints}

We define the decision binary variables $x_{ij}$ which indicate whether the operation $o_i$ is allocated to core $p_j$ or not. Expression \ref{sched:const_1} gives the constraint that each operation has to be allocated to one and only one core.

\begin{equation}
\forall\ o_i \in V, \sum_{p_j \in P}x_{ij}=1
\label{sched:const_1}
\end{equation}

The end time of each operation $o_i$ is computed using the expression \ref{sched:const_2}

\begin{equation}
\forall o_i \in V, E(o_i) = S(o_i) + C(o_i)
\label{sched:const_2}
\end{equation}

For operations that are allocated to the same core and that are completely independent, i.e. no path exists between them, we have to ensure that they are executed in non overlapping time intervals. Expressions \ref{sched:const_11} and \ref{sched:const_12} capture this constraint. $b_{ij}$ is a binary variable that is set to one if $o_i$ is executed before $o_j$.

\begin{equation}
\forall p \in P, \forall\ o_i \in V, \forall\ o_j \in V, (o_i,o_j), (o_j,o_i) \notin A_F, E(o_i) \leq S(o_j) + M \times (3 - x_{ip} - x_{jp} - b_{ij}) 
\label{sched:const_11}
\end{equation}

\begin{equation}
\forall p \in P, \forall\ o_i \in V, \forall\ o_j \in V, (o_i,o_j), (o_j,o_i) \notin A_F, E(o_j) \leq S(o_i) + M \times (2 - x_{ip} - x_{jp} + b_{ij}) 
\label{sched:const_12}
\end{equation}

The cost of synchronization is taken into account according to the synchronization model described in Chapter \ref{ch:3-state}. In other words, a synchronization cost is introduced in the computation of the start time of an operation $o_j$, if it has a predecessor that is allocated to a different core and if its start time is the earliest among the successors of this predecessor that are allocated to the same core as the operation $o_j$. $sync_{ijp}$ is a binary variable which indicates whether synchronization is needed between $o_i$ and $o_j$ if $o_j$ is allocated to $p$. Therefore, $sync_{ijp} = 1\ \text{iff}\ \alpha(o_j)=p\ \text{and}\ \alpha(o_i)\neq p\  \text{and}\ S(o_j) = max_{o_{j'} \in succ(o_i)\ \text{and}\ \alpha(o_{j'}) = p}(S(o_{j'}))$. Expressions \ref{sched:const_3} and \ref{sched:const_4} capture this constraint. $V_{ip}$ is a binary variable that is set to one only if $\alpha(o_i) \neq p$. It is used to define for which cores a synchronization is needed between $o_i$ and its successors, in other words, if the successor is allocated to the same core as $o_i$, no synchronization is needed. Expressions \ref{sched:const_5} and \ref{sched:const_6} capture this constraint. Variable $Q_{ip}$ denotes the earliest start time among the start times of all successors of $o_i$ that are allocated to processor $p$. It is computed using expressions \ref{sched:const_7} and \ref{sched:const_8}. 


\begin{equation}
\forall o_i \in V, \sum_{\forall p \in P, \forall o_j \in pred(o_i)}sync_{ijp}= V_{ip}
\label{sched:const_3}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), sync_{ijp} \leq x_{jp}: \forall o_i \in V
\label{sched:const_4}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), V_{ip} \geq x_{jp} - x_{ip}: \forall o_i \in V
\label{sched:const_5}
\end{equation}

\begin{equation}
 \forall o_i \in V, V_{ip} \leq \sum_{\forall o_j \in succ(o_i)}\big(x_{jp} - x_{ip}\big)
\label{sched:const_6}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), Q_{ip} \leq S(o_j) + M \times (1-x_{jp})
\label{sched:const_7}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), Q_{ip} \geq S(o_j) - M \times (1-sync_{ijp})
\label{sched:const_8}
\end{equation}

The start time of each operation is computed using expression \ref{sched:const_9}. The synchronization cost is introduced taking into account the synchronizations with all predecessors of $o_j$ that are allocated to different cores. 

\begin{equation}
\forall o_j \in V, \forall o_i \in pred(o_j), S(o_j) \geq \Big[E(o_i) + \sum_{\forall p \in P, \forall o_{i'} \in pred(o_j)}sync_{ijp}\times synCost\Big]
\label{sched:const_9}
\end{equation}

The makespan is equal to the latest end time among the end times of all the operations as captured by expession \ref{sched:const_10}

\begin{equation}
\forall o_i \in V, P \geq E(o_i) 
\label{sched:const_10}
\end{equation}

\subsubsection{Objective function}

The objective of this linear program is to minimize the makespan of the dependence graph.

\begin{equation}
min(P)
\label{sched:obj}
\end{equation}


\subsection{Multi-core Scheduling Heuristic}

Multi-core scheduling problems are known to be NP-hard resulting in exponential resolution times when exact algorithms are used. Heuristics have been extensively used in order to solve multi-core scheduling problems. In most situations they lead to results of good quality in practicle resolution times. In particular, list heuristics presented in Chapter \ref{ch:2-bkgnd} are widely used in the context of offline multi-core scheduling.

A variety of list multi-core scheduling heuristics exist in the literature and each heuristic may be suitable for some specific kinds of multi-core scheduling problems. We detail in this section a heuristic that we have chosen to apply on the final graph $G_F(V_F,A_F)$ in order to minimze its makespan. Because of the number of fine-grained operations, and since the execution times and the dependence between the operations are known before runtime, it is more convenient to use an offline scheduling heuristic which has the advantage of introducing lower overhead than online scheduling heuristics. We use an offline scheduling heuristic similar to the one proposed in \cite{grandpierre:1999} which is a fast greedy algorithm whose cost function corresponds well to our minimization objective. In accordance with the principle of list scheduling heuristics, this heuristic is priority-based, i.e. it builds a list of operations that are ready to be scheduled, called candidate operations and selects one operation based on the evaluation of the cost function. We denote by $\rho$ the cost function and call it the schedule pressure. It expresses the degree of criticality of scheduling an operation. The schedule pressure of an operation is computed using its flexibility and the penalty of scheduling which refers to the increase in the critical path resulting from scheduling an operation.  

The heuristic considers the different timing attributes of each operation $o_i \in V_F$ in order to compute a schedule that minimizes the makespan of the graph. The heuristic schedules the operations of the graph $G_F(V_F,A_F)$ on the different cores iteratively and aims at minimizing the schedule pressure of an operation on a specific core while taking into account the synchronization costs. %Let $P^{n}(o_i,p_j)$ be the schedule pressure 
The heuristic updates the set of candidate operations to be scheduled at each iteration. An operation is added to the set of candidate operations if it has no predecessor or if all its predecessors have already been scheduled. For each candidate operation, the schedule pressure is computed on each core and the operation is allocated to its best core, the one that minimizes the pressure. Then, a list of candidate operation-best core pairs is obtained. Finally, the operation with the largest pressure on its best core is selected and scheduled. Synchronization operations are added between the scheduled operation and all its predecessors that were allocated to different cores. The heuristic repeats this procedure and finally stops when all the operations have been scheduled.   

\begin{algorithm}[!htp]		
	  Initialization\;
		Set $\Omega$ the set of all the operations\;  
		Set $P$ the set of all the available cores\; 
 		Set $O$ the set of operations without predecessors\;  		
 		\While{$O \neq \emptyset$}
		{
 			\ForEach{operation $o_i \in O$} 
			{
 				Set $\sigma$ to $\infty$; (cost of $o_i$ is set to the maximum value)\;
 				\ForEach{$p \in P$}
				{
  				$S'(o_i) \leftarrow max(S(o_i) , L_p)$; (new start time of $o_i$ when executed on $p$)\;
  				$\sigma' \leftarrow S'(o_i) + C(o_i) + \overline{E}(o_i) - R$; (cost of $o_i$ when executed on $p)$\;
  				\If{$\sigma' < \sigma$}
					{
  					Set $\sigma \leftarrow \sigma'$\;
  					Set $BestCore(o_i) \leftarrow p$\;
  				}
  			}
  		 }
  		 Find $o_{i'}$ with maximal cost $\sigma$ in $O$\; 
  		 Schedule $o_{i'}$ on its core $\mathrm{BestCore}(o_{i'})$\;
  		 Set $p' := \mathrm{BestCore}(o_{i'})$\;
  		 $L_{\mathrm{p'}} := L_{\mathrm{p'}} + C(o_{i'})$; (Advance the time of $p'$)\;
  		 Remove $o_{i'}$ from the set $O$\;
  		 Add to the set $O$ all successors of $o_{i'}$ for which all predecessors are already scheduled\;
		}
	\caption{Multi-core scheduling heuristic}
	\label{algo:sched}
\end{algorithm}

\subsubsection{Complexity}

The scheduling heuristic contains three nested loop. The outermost loop is executed until all the operations are scheduled. At each iteration, one operation is scheduled. Therefore, the outermost loop is executed $n$ times where $n$ is the number of operations in the operation graph. In the inner loops, the heuristic attempts to schedule all the ready operations on all the available cores. As such, the inner loops execute in $\mathcal{O}(np)$, where $p$ is the number of cores. From the foregoing, the complexity of our heuristic is evaluated to $\mathcal{O}(pn^2)$.

\section{Scheduling of FMU Co-simulation with Real-time Constraints}

In this section, we are interested in scheduling FMU co-simulation while ensuring a set of real-time constraints  is satisfied. We consider FMU co-simulation in the context of HiL consisting of a simulated component and a real component. Also, we consider that the real-time constraints that are applied by the real component have been propagated through the operation graph as described in Section \ref{sec:grphrtsc}. Therefore, the aim here consists in scheduling the operations of the operations graph on a multi-core architecture, such as these constraints are satisfied. Note that in contrast to the previous section, how much the execution is sped up is not of a crucial important here as long as the real-time constraints are respected. Hereafter, we present an ILP formulation and a heuristic for scheduling operation graphs under real-time constraints.

\subsection{Problem Formulation}

The problem of scheduling FMU co-simulation under real-time constraints can be considered as a satisfaction problem instead of an optimization problem. In fact, the basic problem does not involve an objective function to be optimized, the goal being to ensure the real-time constraints are satisfied. More precisely, the problem consists in scheduling the operations of the operation graph such that each operation starts its execution no earlier than its release date and finishes its execution by its deadline date. Obviously, there are other constraints that are common with the acceleration problem, namely, respecting the partial order of the operation graph. Also, the cost of inter-core synchronization is taken into account in computing the schedule. The scheduling of FMU co-simulation under real-time constraints can be stated as a satisfaction problem as follows:

\begin{table}[h]
\centering
\begin{tabular}{l  l}
  \rule{0pt}{5ex}	
	\textit{Input} & Operation graph $G_F(V_F,A_F)$\\
	\rule{0pt}{5ex}									  
	
  \textit{Output} & Offline Schedule of operations on multi-core processor\\
	\rule{0pt}{5ex}									  
  
	\textit{Find} & Allocation of operations to cores, $\alpha: V \rightarrow P$\\
	\rule{0pt}{5ex}
                & Assignment of start times to operations, $\beta: V \times P \rightarrow \mathbb{N}$\\
	\rule{0pt}{5ex}								 
	
	\textit{Subject to} & Precedence constraints of the graph $G_F(V_F,A_F)$\\
											& Start time greater or equal to the release date $\forall o_i \in V_F, S(o_i) \geq R(o_i)$\\
											& End time less or equal to the deadline date $\forall o_i \in V_F, E(o_i) \leq D(o_i)$\\
										 
	
\end{tabular}
\end{table}

\subsection{Accounting for Dependence in Real-time Scheduling}

The model of computation for (co-)simulation is close to the synchronous paradigm \cite{benveniste:1991synchronous,benveniste:2003}. In this paradigm, a program evolves according to a sequence of ticks of logical time at which computations are considered to produce their results instantaneously. The propagation of the release and deadline constraints presented earlier follows this model of computation. However, when real-time constraints are involved, co-simulation becomes incompatible with the synchronous paradigm. In fact, each operation takes a certain execution time to run and, therefore, cannot produce the result instantaneously. Consequently, the execution times of the operations have to be accounted for when assigning release and deadline dates to the operations.

We adopt an approach similar to the one proposed in \cite{chetto:1990} to modify the release and deadline dates assigned to each operation in order to account for execution times. This modification is needed given that:

\begin{itemize}

\item The execution of an operation can start no earlier than its release but also only after the execution of all its predecessors is finished.

\item The execution of an operation must be finished before its deadline and also be finished so that the execution of of its successors can be finished before their deadlines.

\end{itemize}

Let $o_i$ and $o_j$ be two operations such that $o_j \in pred(o_i)$. For a given schedule of the operation graph to be valid, the relations $S(o_i) \geq R(o_i)$ and $S(o_i) \geq E(o_j)$ must be satisfied. Therefore, the new release date of $o_i$ can be computed by expression \ref{eq:modre}.

\begin{equation} 
R(o_i) = max(R(o_i), max(F(o_j), o_j \in pred(o_i)))
\label{eq:modre}
\end{equation}

Consider now the two operations $o_i$ and $o_j$ such that $o_j \in succ(o_i)$. For the operation graph to be schedulable, the relations $F(o_i) \leq D(o_i)$ and $F(o_i) \leq D(o_j) - C(o_j)$ must be satisfied. In fact, $D(o_j) - C(o_j)$ represents the latest time to start the execution of the successor $o_j$ and meet its deadline. Therefore, the new deadline date of $o_i$ can be computed by expression \ref{eq:modde}.

\begin{equation} 
D(o_i) = min(D(o_i), min(D(o_j) - C(o_j), o_j \in succ(o_i)))
\label{eq:modde}
\end{equation}   

\subsection{Resolution using Linear Programming}

The ILP formulation that we propose is, in most part, similar the ILP formulation for co-simulation acceleration. The main differences consist, first, in adding the inequalities that express the real-time constraints. Second, it is possible not to set an objective function since the real-time scheduling consists in a satisfaction problem.

Therefore, the ILP formulation for multi-core scheduling of co-simulation under real-time constraints is given below. The start date of every operation must be at the earliest equal to its release date. Expression \ref{schedRT:const_01} captures this constraint. The deadline date of every operation is the latest time before which the operation has to finish its execution. Expression \ref{schedRT:const_02} specifies this constraint.

\begin{equation}
\forall\ o_i \in V, \sum_{p_j \in P}x_{ij}=1
\label{schedRT:const_1}
\end{equation}

\begin{equation}
\forall o_i \in V, E(o_i) = S(o_i) + C(o_i)
\label{schedRT:const_2}
\end{equation}

\begin{equation}
\forall\ o_i \in V, S(o_i) \geq R(o_i)
\label{schedRT:const_01}
\end{equation}

\begin{equation}
\forall\ o_i \in V, E(o_i) \leq D(o_i)
\label{schedRT:const_02}
\end{equation}

\begin{equation}
\forall p \in P, \forall\ o_i \in V, \forall\ o_j \in V, (o_i,o_j), (o_j,o_i) \notin A_F, E(o_i) \leq S(o_j) + M \times (3 - x_{ip} - x_{jp} - b_{ij}) 
\label{schedRT:const_11}
\end{equation}

\begin{equation}
\forall p \in P, \forall\ o_i \in V, \forall\ o_j \in V, (o_i,o_j), (o_j,o_i) \notin A_F, E(o_j) \leq S(o_i) + M \times (2 - x_{ip} - x_{jp} + b_{ij}) 
\label{schedRT:const_12}
\end{equation}

\begin{equation}
\forall o_i \in V, \sum_{\forall p \in P, \forall o_j \in pred(o_i)}sync_{ijp}= V_{ip}
\label{schedRT:const_3}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), sync_{ijp} \leq x_{jp}: \forall o_i \in V
\label{schedRT:const_4}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), V_{ip} \geq x_{jp} - x_{ip}: \forall o_i \in V
\label{schedRT:const_5}
\end{equation}

\begin{equation}
 \forall o_i \in V, V_{ip} \leq \sum_{\forall o_j \in succ(o_i)}\big(x_{jp} - x_{ip}\big)
\label{schedRT:const_6}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), Q_{ip} \leq S(o_j) + M \times (1-x_{jp})
\label{schedRT:const_7}
\end{equation}

\begin{equation}
\forall o_i \in V, \forall o_j \in succ(o_i), Q_{ip} \geq S(o_j) - M \times (1-sync_{ijp})
\label{schedRT:const_8}
\end{equation} 

\begin{equation}
\forall o_j \in V, \forall o_i \in pred(o_j), S(o_j) \geq \Big[E(o_i) + \sum_{\forall p \in P, \forall o_{i'} \in pred(o_j)}sync_{ijp}\times synCost\Big]
\label{schedRT:const_9}
\end{equation}


\subsection{Multi-core Scheduling Heuristic}

Removing the objective function from the scheduling problem might indicate that the complexity of the scheduling problem is reduced in the real-time case compared to the acceleration case. However, adding the strict release and deadline constraints adds to the complexity of the problem which remains an NP-Hard problem that is equivalent to the bin packing problem.

In the following, we propose a heuristic for scheduling operation graphs representing FMU co-simulations under real-time constraints. There are some considerations that are common with the scheduling problem for co-simulation acceleration. Mainly, we propose an offline heuristic which we consider to be more suitable given the fine granularity of the operations and since information about the execution times of the operations and the dependence between them is available before runtime.

We propose to adapt the scheduling heuristic that we use for the acceleration of FMU co-simulation. In particular, we modify the computation of the scheduling priority such that the criticality of a given operation expresses how close it is to miss its deadline if scheduled on a specific processor. The priority of an operation is a dynamic operation as its computation depends on the partial scheduling solution that has already been computed. This priority is given by expression \ref{eq:priority}.

\begin{equation}
\rho_{i,j} = D(o_i) - E_j(o_i)
\label{eq:priority}
\end{equation}

Where $\rho_{i,j}$ and $E_j(o_i)$ are the scheduling priority of operation and the end date of operation $o_i$ respectively, computed when the latter is scheduled on core $j$.

The proposed heuristic is a list scheduling heuristic. It builds the multi-core schedule iteratively. At each iteration, a list of candidate operations is constructed. An operation is added to the list of candidate operation if all its predecessors have been scheduled. The heuristic computes the priority for each candidate operation on every core and selects the one for the which the priority is maximized. After that, a list of pairs operation-best core is obtained. The heuristic selects from this list the operation whose priority is the least among all operation in the list. Synchronization operations are added between the scheduled operation and all its predecessors that were allocated to different cores. The heuristic repeats this procedure and finally stops when all the operations have been scheduled. Algorithm \ref{algo:schedRT} lists the proposed real-time multi-core scheduling heuristic. 

\begin{algorithm}[!htp]		
	  Initialization\;
		Set $\Omega$ the set of all the operations\;  
		Set $P$ the set of all the available cores\; 
 		Set $O$ the set of operations without predecessors\;  		
 		\While{$O \neq \emptyset$}
		{
 			\ForEach{operation $o_i \in O$} 
			{
 				Set $\sigma$ to $-\infty$; (cost of $o_i$ is set to the minimum value)\;
 				\ForEach{$p \in P$}
				{
  				$S'(o_i) \leftarrow max(S(o_i) , L_p)$; (new start time of $o_i$ when executed on $p$)\;
  				$\sigma' \leftarrow D(o_i) - E(o_i)$; (priority of $o_i$ when executed on $p)$\;
  				\If{$\sigma' > \sigma$}
					{
  					Set $\sigma \leftarrow \sigma'$\;
  					Set $BestCore(o_i) \leftarrow p$\;
  				}
  			}
  		 }
  		 Find $o_{i'}$ with least priority $\sigma$ in $O$\; 
  		 Schedule $o_{i'}$ on its core $\mathrm{BestCore}(o_{i'})$\;
  		 Set $p' := \mathrm{BestCore}(o_{i'})$\;
  		 $L_{\mathrm{p'}} := L_{\mathrm{p'}} + C(o_{i'})$; (Advance the time of $p'$)\;
  		 Remove $o_{i'}$ from the set $O$\;
  		 Add to the set $O$ all successors of $o_{i'}$ for which all predecessors are already scheduled\;
		}
	\caption{Multi-core scheduling heuristic}
	\label{algo:schedRT}
\end{algorithm} 

\subsection{Scheduling Interval}

In offline scheduling, the schedule is computed over an interval of time. This schedule is then executed repetitively. For the acceleration of co-simulation, we have seen that the length of the schedule interval is equal to the Hyper-Step. For co-simulation under real-time constraints, a natural approach is to apply techniques that are used for classical real-time systems (such co-simulation is considered a real-time system after all). For this, we need first to represent the operation graph with a model that involves the parameters that are usually used for classical real-time systems. In particular, we need to define a relative deadline and a period for each operation. Note that, so far, we have only spoken about sampling periods of data exchange between the real and the simulated component. Although related to the sampling periods, the periods that we seek to define for each operation are different and correspond to task periods that are found in classical real-time systems. We handle this requirement as follows. We consider that every operation that appears in the Hyper-Period pattern of the operation graph is a distinct operation. In other words, occurrences of one operation are not regarded as repetitions of a single operation. Accordingly, we consider that the operation graph is \textit{mono-period}, i.e. all the operations have the same period. The value of this period is equal to the Hyper-Period. The relative deadline of each operation can then be defined as the duration between its release and deadline.   

The length of the schedule interval for co-simulation under real-time constraints cannot be chosen in a straightforward manner to be equal to the Hyper-Period. This is because the operation graph features \textit{arbitrary deadlines}, i.e. relative deadlines that are greater than the periods.

In the real-time literature, we find contributions regarding the schedule interval targeting different kinds of real-time tasks, schedulers, and architectures. For instance, in \cite{cucu:2006}, the authors study synchronous task systems, i.e. where the release dates of all tasks are equal to zero, with constrained deadlines, i.e. where the relative deadline of each task is less or equal to its period. They show that the schedule of such task system on uniform multiprocessors reaches a cyclic behavior after one Hyper-Period. In \cite{cucu:2007}, a schedule interval is given for preemptive scheduling of tasks with arbitrary deadlines on uniform multiprocessors. A more general result is given in \cite{grolleau:2013} taking into account different constraints (mutual exclusion, precedence constraints, non-preemptive tasks, etc.) for uniprocessor and multiprocessor scheduling. The authors give an upper bound for the schedule interval when a \textit{deterministic memoryless} scheduler is used. A scheduler is deterministic and memoryless if and only if, when building the schedule, the scheduling decision is the same for any identical configuration encountered. The given bound is:

\begin{equation} 
\prod_{i=1}^n((\max(O_i + D_i - T_i, 0) + 1)) \times H
\end{equation}

where $n$ is the number of tasks, $O_i$, $D_i$, and $T_i$ are the offset (release date), relative deadline, and period of task $\tau_i$ respectively, and $H$ is the Hyper-Period. This result is applicable to non-preemptive scheduling with arbitrary deadlines which is the case in out problem. However, the proposed bound is intractable, i.e. as the size of the operation graph grows, it results in very large schedule intervals and we cannot guarantee to compute the schedule within an acceptable time.

%Add here how we deal with the problem

As shown in Section \ref{sec:grphrtsc}, the propagation of the real-time constraints in an operation graph may lead to assigning negative release dates to some operations. This means that such operations can be executed before launching the co-simulation under real-time constraints. Therefore, they are not scheduled and are removed from the first repetition of the operation graph. See, for example, the operations colored in gray in Figure. But since these operations must appear in the subsequent repetitions of the operation graph and the schedule, we have to add the repetitions of these operations that belong to the second repetition of the operation graph to the operation graph pattern that is scheduled. See Figure.

\section{Code Generation}

In this section, we describe how the FMU co-simulation code is generated based on the schedule tables produced by the proposed scheduling algorithms. Note that, while the schedule tables are produced using different algorithms, the code generation is done in a similar way for both acceleration of co-simulation and co-simulation under real-time constraints. Since the FMU co-simulation is intended to be executed on multi-core desktop computers running general purpose or real-time operating systems, the implementation is achieved using \textit{native} threads. Such threads consist in threads that are provided by the operating system in contrast to threads that are related to a specific programming language and/or rely on a specific runtime library.

In the generated code, as many threads are created as there are cores. Each thread is responsible for the execution of the schedule of one core. Therefore, each thread reads from the schedule table of its corresponding core and executes the operations that are specified in this table. These operations can be computational operations, i.e. input, output, and state operations, or synchronization operations. The synchronization operations are implemented using semaphores provided by the operating system. They are of two types: \textit{signal} and \textit{wait} operations. The execution of a signal operation by a thread consists in signaling the corresponding semaphore. The execution of a wait operation by a thread consists in block waiting for the corresponding semaphore. Each thread executes its associated schedule table repeatedly, and thus executes FMU operations and synchronizes with the other threads. Hereafter, we refer to these threads as \textit{schedule threads}. 

The orchestration of the co-simulation is ensured by a \textit{master} thread which runs the FMI master algorithm. The master thread creates and launches the schedule threads. During the execution, the master thread and the schedule threads are synchronized at fixed points. First, the master thread signals to the schedule threads the start of the co-simulation which launches their execution. Each thread starts, then, the execution of its associated schedule table as described in the previous paragraph. When it finishes the execution of the whole schedule table, its signals this to the master thread and waits for a new signaling from it. The master thread block waits until all the schedule threads signal that they finished the execution of their respective schedule tables. Then, the master thread launches a new iteration by signaling to the schedule threads to start executing thir corresponding schedule tables again. This process is repeated until the desired simulation time is reached. Figure \ref{fig:codegen} shows an example of the execution of the generate code for an FMU co-simulation on a two core processor.

\begin{figure}[phbt]
\centering
\includestandalone{figures/codegen}
\caption{Illustration of generated code.}
\label{fig:codegen}
\end{figure}   


