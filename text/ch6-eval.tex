\chapter{\label{ch:6-eval}Evaluation}

\minitoc

In this chapter, we evaluate our proposed approach. We start by describing a method for randomly generating benchmark operation graphs. Then, we present the evaluation of the performances of the acyclic orientation and the scheduling heuristics. Finally, we give runtime performance and numerical accuracy results obtained by applying our approach on an industrial use case.

\section{Random Generator of Operation Graphs}

Due to the difficulty in acquiring enough industrial FMU co-simulation applications for assessing our approach, we had to use a random generator of FMU dependence graphs. The generator creates the graphs and characterizes them with attributes.

\subsection{Random Operation Graph Generation}

The random generator that we have implemented is inspired by the random generator presented in \cite{kalla:2004}. However, it differs from this work in that the generation is done in two stages. In other words, we have to generate, first, the different FMUs of the co-simulation and their internal structures. Second, we generate the dependence graph by creating inter-FMU dependence in such a way that the resulting operation graph is a DAG. The proposed generator is based on a technique of assignment of operations to levels. The level of an operation is the number of operations on the longest path from a source operation to this operation. The dependence graph can then be visualized on a grid of levels as depicted in Figure \ref{fig:genexample}. The generator uses the following parameters:

\begin{itemize}
\item The graph size $n$: the number of operations;
\item The number of FMUs $m$;
\item The graph height $h$: the maximum number of levels in the graph;
\item The graph width $w$: the maximum number of nodes on one level
\end{itemize}

Note that parameters $n$ and $m$ are related. In other words, for a given size of a graph $n$, an adequate number of FMUs $m$ has to be chosen. 

The generation of the dependence graph is performed as follows:
\begin{itemize}
\item \textbf{Input:} Size of the graph $n$, number of FMUs $m$, height of the graph $h$, and width of the graph $w$. %The number of FMUs can be derived automatically from the size of the graph using a predefined formula. For instance, one of the formulas that we have used is $ m = 5 \times log10(n/5)$. This allows to have adequate size of the graph and number of FMUs. Suppose, for example, that we have a size $n = 1000$ and a number of FMUs $m = 1$. Obviously, this example does not represent a realistic application. 
\item \textbf{Step 1:} Randomly distribute the $n$ operations across the $m$ FMUs. Given the number of operations of each FMU, we randomly determine the number of its input operations and the number of its output operations. Every FMU has one state operation.
\item \textbf{Step 2:} Randomly generate the intra-FMU arcs. This step is controlled by two parameters.  The number of arcs to generate and the number of NDF outputs of the FMU. These outputs are not considered when randomly generating the arcs.
\item \textbf{Step 3:} Randomly assign the operations to the grid levels. This step is performed by assigning output operations and then input operations repeatedly.
\begin{enumerate}
\item Assign all NDF operations to level $0$ of the grid.
\item Randomly assign remaining output operations to even levels $(2, 4, \ldots, h-3)$ of the grid.
\item Assign the input operations to the odd levels $(1, 3, \ldots, h-4)$ of the grid such that any input operation $o_i$ that is connected to an output operations $o_j$ (intra-FMU dependence) is assigned to the level preceding the level to which $o_j$ has been assigned. 
\item Assign the remaining input operations (each of which is not connected with any output operation) to the level $h-2$ of the grid. These operations will be connected only with the state operations of their respective FMUs.
\item Add the state operations to the last level of the grid.
\end{enumerate}
\item \textbf{Step 4:} Create the arcs of the dependence graph. At this step, we randomly generate inter-FMU dependence. For each operation $o_i$ on the level $lvl$ of grid, we randomly select an output operation $o_j$ from the preceding level $lvl-1$ and which belongs to a different FMU than $o_i$. We create an arc from $o_j$ to $o_i$. If no such output operation is found at level $lvl-1$, we select randomly an output operation from any level $lvl' < lvl-1$ and connect it with the operation $o_i$. Finally the arcs from input and output operations to state operations are created. 
\end{itemize}

Figure \ref{fig:genexample} illustrates the steps of our proposed random operation graph generator.

\begin{figure}[phbt]
\centering
\includestandalone{figures/genexample}
\caption{Random generation of an operation graph.}
\label{fig:genexample}
\end{figure} 

\subsection{Random Operation Graph Characterization}

In addition to random generation of the dependence graph structure, we need to generate the attributes of the graph. In particular, the following attributes are generated by our random generator:
\begin{itemize}
\item Communication steps of the FMUs: A range or a set for the values of the communication steps is specified. The generator randomly assigns a communication step from this range or set to every FMU. 
\item Execution times of the operations: Different ranges of the execution times are specified for input, output, and state operations. Execution times are generated randomly in such a way that state operations have longer execution times than output and input operations.
\end{itemize}

\section{Performances of the Heuristics}

We have carried out different tests in order to evaluate our proposed approach. For both the acyclic orientation and the scheduling, we compared the execution time of our proposed heuristic with the execution time of the ILP, and the value of the objective function of the heuristic with the value of the objective function of the ILP. For ILP resolution, we used three solvers: lpsolve \cite{berkelaar:2004}, Gurobi \cite{gurobi:2016}, and CPLEX \cite{cplex:2017}. With lpsolve, we were only able to solve small instances of the scheduling problem. Gurobi was much more efficient but we obtained the best performance using CPLEX. Therefore, the results presented hereafter were obtained using CPLEX. %Tests have been carried out on different multi-core PCs. In the following, we indicate, when necessary, the characteristics of the platform used.  
Tests were performed on a desktop computer with a 6-core Intel Xeon processor running at 2.7 GH and 16GB RAM.

\subsection{Execution Time of the Acyclic Orientation Algorithms}

In order to compare the execution time of our acyclic orientation heuristic with the execution time of the acyclic orientation ILP, we have generated 200 random operation graphs of different sizes between five and $10000$. %First we generated the first operation graph of size $n = 5$. Then, we generated the rest of the graphs by varying the size of the operation graph by different increment values. The size of the graph is increased by five until a size of $100$, i.e. $n = 5, 10, 15, \ldots, 100$.
We considered $10000$ as the maximum size of the operation graph because it corresponds to the size of large industrial applications. 

We executed the acyclic orientation heuristic and ILP on all of the generated random graphs and measured the elapsed time between the start and the end of the execution. For the ILP, the execution is stopped if the optimal solution is not found within two days. The obtained execution times are shown on a logarithmic scale in Figure \ref{fig:orient_exec}. The acyclic orientation ILP cannot be resolved in practical times when the size of the operation graph exceeds $250$. When The number of operations is less than $250$ the ILP finds the optimal solution in reasonable times, except for two graphs. In addition, we observe that an increase in the graph size does not always result in an increase in the execution time. This can be explained by the fact that other factors impact the speed of resolution, e.g. number of conflict edges. Still, it is important to notice that the application of the acyclic orientation ILP is limited to relatively small graphs. On the other hand, the acyclic orientation heuristic produces results in practical execution times even for very large operation graphs ($10000$).

\begin{figure}[phbt]
\centering
\includestandalone{figures/orient_exec}
\caption{Comparison of the execution times of the acyclic orientation algorithms.}
\label{fig:orient_exec}
\end{figure} 
 
\subsection{Critical Path Length}

We compared the values of the critical path length obtained using the acyclic orientation heuristic and ILP. Tests were performed using the same set of operation graphs described in the previous section. However, we consider only graphs for which the ILP was able to return the optimal solution within the resolution time limit that we set, i.e. two days. Thus, we applied our proposed heuristic and ILP on 12 operation graphs of sizes between $20$ and $240$ and saved the obtained length of the critical path. Results are depicted in Figure \ref{fig:orient_critpath}. For most of the operation graphs, our acyclic orientation heuristic produces a length of the critical path that is equal to the length of the critical path produced by the acyclic orientation ILP. The heuristic returns a longer length of the critical path for three graphs but the gap is very small remaining below $8\%$.  

\begin{figure}[phbt]
\centering
\includestandalone{figures/orient_critpath}
\caption{Comparison of the critical path length.}
\label{fig:orient_critpath}
\end{figure}

\subsection{Execution Time of the Scheduling Algorithms for Co-simulation Acceleration}

Similarly to the acyclic orientation tests, we compared the execution time of the scheduling heuristic with the execution time of the scheduling ILP using 200 generated random operation graphs of different sizes between five and $10000$. We set a two day limit for the resolution of the ILP. Tests were run for the scheduling problem with $2$, $4$, and $8$ cores. Execution times were measured by fixing the number of cores and varying the number of operations (graph size). The results are depicted for $2$, $4$, and $8$ cores in Figure \ref{fig:sched_exec_2}, Figure \ref{fig:sched_exec_4}, and Figure \ref{fig:sched_exec_8} respectively. All results are plotted on a logarithmic scale. In these figures, we see that the execution time of the ILP resolution increases exponentially as the graph size increases, and only small instances are resolved within acceptable times. On the other hand, the acyclic orientation heuristic is very fast and produces results in very short times and even for very large graphs, the execution times remain within practical bounds.

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_exec_2}
\caption{Comparison of the scheduling execution time for $2$ cores.}
\label{fig:sched_exec_2}
\end{figure}

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_exec_4}
\caption{Comparison of the scheduling execution time for $4$ cores.}
\label{fig:sched_exec_4}
\end{figure}

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_exec_8}
\caption{Comparison of the scheduling execution time for $8$ cores.}
\label{fig:sched_exec_8}
\end{figure}

\subsection{Makespan}

We run tests to compare the value of the makespan obtained using the acyclic orientation heuristic and ILP. For these tests we have generated ten operation graphs of size $n = 15$. We have used graphs of size $15$ because the ILP resolution returns the optimal solution in very short times which is not the case for large graphs. The graphs are different from each other because they are generated randomly which leads to different topologies and execution times of the operations. We run the scheduling heuristic and ILP on these graphs to obtain the values of the makespan. Results are shown in Figure \ref{fig:sched_mkspan_2}, Figure \ref{fig:sched_mkspan_4}, and Figure\ref{fig:sched_mkspan_8} for $2$, $4$, $8$ cores respectively. Overall, the results show that the scheduling heuristic produces a makespan which is very close to the makespan produced by the scheduling ILP. The gap between the heuristic and the ILP result lies between $0\%$ and $16\%$. We notice that the gap is smaller when $4$ or $8$ cores are used than when $2$ cores are used. In fact, when $2$ cores are used the maximum gap is $16\%$, whereas when $4$ or $8$ cores are used the maximum gap is $6\%$. This shows that the scheduling heuristic performs better when the effective parallelism is increased. It can be explained by the fact that the scheduling heuristic attempts more allocation possibilities which leads to a better exploitation of the potential parallelism.

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_mkspan_2}
\caption{Comparison of the makespan for $2$ cores.}
\label{fig:sched_mkspan_2}
\end{figure}

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_mkspan_4}
\caption{Comparison of the makespan for $4$ cores.}
\label{fig:sched_mkspan_4}
\end{figure}

\begin{figure}[phbt]
\centering
\includestandalone{figures/sched_mkspan_8}
\caption{Comparison of the makespan for $8$ cores.}
\label{fig:sched_mkspan_8}
\end{figure} 

%\subsection{Execution of the Co-simulation}
%
%The results presented in this section show how the acyclic orientation and the scheduling heuristics perform in terms of execution time and optimality gap. In the next section, we evaluate the runtime performance, i.e. the execution of the co-simulation code that is generated based on the result of the scheduling heuristic. To this end, we applied our proposed heuristics on an industrial use case.

\subsection{Execution Time of the Scheduling Algorithms for Co-simulation under Real-time Constraints}

\subsection{Schedulability Rate}

\section{Industrial Use Case} 

We tested our proposed approach on an industrial use case. Tests have been performed on a computer with an 8-core Intel core i7 processor running at 2.7 GH with 16GB RAM. In the rest of this section, we first give a description of the use case and then present the tests and the obtained results.

\subsection{Use Case Description}

Our use case consists in a Spark Ignition (SI) RENAULT F4RT engine co-simulation. It is a four-cylinder in line Port Fuel Injector (PFI) engine in which the engine displacement is 2000 $cm^3$. The air path is composed of a turbocharger with a mono-scroll turbine controlled by a waste-gate, an intake throttle and a downstream-compressor heat exchanger (Figure \ref{fig:use_case}). This co-simulation is composed of six FMUs: an FMU of the airpath, four FMUs of the four cylinders, and one FMU of the controller.
The engine model was developed using ModEngine library \cite{benjelloun:2011}. ModEngine is a Modelica library that allows for the modeling of a complete engine with diesel and gasoline combustion models. The engine model was imported into xMOD using the FMI export features of the Dymola\footnote{http://www.3ds.com/products-services/catia/products/dymola} tool. The operation graph of this use-case contains over 100 operations.

\begin{figure}[phbt]
\centering
\includestandalone{figures/use_case}
\caption{Spark Ignition (SI) RENAULT F4RT engine model.}
\label{fig:use_case}
\end{figure}

\subsection{Test Campaign}

We based our tests on three different versions of RCOSIM. We refer to our proposed method as MUO-RCOSIM (for Multi-Rate Oriented RCOSIM). We compared the obtained results with two approaches: The first one is RCOSIM which is mono-rate and thus we had to use the same communication step for all the FMUs. We used a communication step of $20 {\mu}s$. The second one consists in using RCOSIM with the multi-rate graph transformation algorithm. We refer to it as MU-RCOSIM (for Multi-Rate RCOSIM). For MUO-RCOSIM and MU-RCOSIM we used the recommended configuration of the communication steps for this use case. For each cylinder, we used a communication step of $20 {\mu}s$. The communication step used for the airpath is $100 {\mu}s$. The airpath has slower dynamics than the cylinders and this configuration of the communication steps corresponds to the specification given by engine engineers. For each FMU, we used a Runge-Kutta 4 solver with a fixed integration step equal to the communication step. The graph of this use case is transformed by Algorithm \ref{algo:mr} into a graph containing over 280 operations that are scheduled by the multi-core scheduling heuristic.      

\subsection{Numerical Accuracy}

The validation of the numerical results of the co-simulation using the proposed method is achieved through the comparison of the co-simulation outputs with reference outputs. Since it is not possible to solve the equations of the FMU analytically, the reference outputs are obtained by using RCOSIM which has been shown in \cite{benkhaled:2014} to give a very good accuracy of the numerical results. Figure \ref{fig:df} shows the obtained results for the torque (an output of the airpath). We note that the results match with the reference, and the generated error is very small remaining within an acceptable bound ($< 1\%$). Similar accuracy results were obtained for the different outputs of the co-simulation.

\begin{figure}[phbt]
\centering
\includestandalone{figures/error_df}
\caption{Numerical results.}
\label{fig:df}
\end{figure}

\subsection{Speedup}

The speedup obtained using MUO-RCOSIM is compared with the speedups obtained using RCOSIM and MU-RCOSIM. The speedup was evaluated by running the co-simulation in xMOD. Execution times measurements were performed by getting the system time stamp at the beginning and at the end of the co-simulation. For a given run of the co-simulation, the speedup is computed by dividing the mono-core co-simulation execution time of RCOSIM by the co-simulation execution time of this run on a fixed number of cores. Figure \ref{fig:spdup} sums up the results. The same speedup is obtained using MUO-RCOSIM and MU-RCOSIM even when only 1 core is used. This speedup is obtained thanks to using the multi-rate configuration. More specifically, increasing the communication step of the airpath from $20 {\mu}s$ to $100 {\mu}s$ results in fewer calls to the solver leading to an acceleration in the execution of the co-simulation. By using multiple cores, speedups are obtained using both MUO-RCOSIM and MU-RCOSIM. Additionally, MUO-RCOSIM outperforms MU-RCOSIM with an improvement in the speedup of, approximately $30\%$ when 2 cores are used, and approximately $10\%$ when 4 cores are used. This improvement is obtained thanks to the acyclic orientation heuristic which defines an efficient order of execution for the operations of each FMU that are mutually exclusive. This defined order tends to allow the multi-core scheduling heuristic to better adapt the potential parallelism of the operation graph to the effective parallelism of the multi-core processor (number of cores) resulting in an improvement in the performance. MU-RCOSIM, on the other hand, uses the solution of RCOSIM which consists in simply allocating mutual exclusive operations to the same core introducing restrictions on the possible solutions of the multi-core scheduling heuristic. When using 8 cores, no further improvement is possible since the potential parallelism is fully exploited. Worse still, the overhead of the synchronization between the cores becomes counter-productive, which explains why the speedup with 8 cores is less than the speedup with 4 cores for all the approaches. The best performance is obtained using 5 cores with slight improvement compared to using 4 cores. 

\begin{figure}[phbt]
\centering
\includestandalone{figures/speedup_uc}
\caption{Speedup results.}
\label{fig:spdup}
\end{figure}  

\subsection{Comparison of Offline and Online Scheduling}

In this thesis, we adopted an offline scheduling heuristic assuming it is more efficient than online scheduling since it introduces lower overhead. This choice was based on the fact that the grain size of the operation graph is small which makes it unsuitable for online scheduling which involves more decision overhead in runtime than offline scheduling. In fact, the decision overhead in runtime may become much more costly then the execution of the operations. Moreover, the different operations perform different tasks and have different execution times in contrast to applications that exhibit data parallelism which could be efficiently handled by online scheduling. In addition, the execution times of the operations and the dependence between them are known before the execution which allows the application of offline scheduling. In order to confirm this assumption, we have compared our approach with a runtime scheduling approach, i.e. online scheduling. For this end, we have used Intel TBB library for the parallelization of the co-simulation. We performed several speedup tests and compared the results obtained using the two approaches.

\subsubsection{Intel TBB Flow Graph}

We have chosen Intel TBB to implement an online scheduling because it offers a programming interface introduced in Intel TBB 4.0, which allows easy parallelization of programs represented as graphs. 
It can be combined with loop parallelism supported by Intel TBB to further improve the parallelism exploitation. 
In Intel TBB, we distinguish between dependence graphs and data flow graphs.
In dependence graphs, a dependence represents a precedence constraint between two nodes. During execution, this dependence acts as a signal to inform a node that a predecessor has finished its execution. 
In data flow graphs, a dependence is accompanied by data transfer from a predecessor to a node.
In our implementation we used dependence graphs as explianed hereafter.
Intel TBB offers a wide range of classes that can be used to implement dependence graphs. In particular the graph class and other related classes are used for this purpose. In general, a dependence graph involves three main components: a graph object, nodes, and edges. A graph object provides methods for the execution of tasks created from the nodes of the graph and to wait for the execution of the dependence graph to finish.
Provided node classes allow the creation of different types of nodes. These nodes can be classified into four categories as shown in Figure \ref{fig:tbbnodes}.

\begin{figure}[phbt]
\centering
\includestandalone{figures/tbbnodes}
\caption{Types of nodes supported by the Intel TBB Flow Graph interface.}
\label{fig:tbbnodes}
\end{figure}

Functional nodes can be used to execute user code provided as a body object. Buffering nodes allow accumulating messages as they flow through the graph and forwarding them to successors. Different buffering protocols are supported by Intel TBB such as FIFO, arbitrary order, or priority order. Split/Joint nodes can be used for aggregation and deaggregation of messages. There exist several other specific purpose node types, e.g. broadcast node. Inputs and outputs of nodes are called ports. %A node is equivalent to an operation according to our definition 
The user creates a graph, its nodes and then specifies dependence between them. In Intel TBB, edges are used to create the dependence. These edges can be created using dedicated functions provided by Intel TBB. Such functions can be used for the creation and removal of edges in the graph, as well as managing ports of nodes.
The classes and functions of Intel TBB are highly parametrized to allow many possibilities of implementation.

The execution of the dependence graph follows the partial order specified by the created edges. When a node receives a signal of completion, a task is spawned to execute the body of this node. 

We present here the fundamental concepts necessary to describe how we used Intel TBB. The official documentation\footnote{software.intel.com/en-us/tbb-reference-manual} of Intel TBB should be consulted for more detailed explanation. 


\subsubsection{Scheduling in Intel TBB}

Intel TBB is based on programming with tasks instead of threads. Tasks are atomic units of execution that are allocated to threads to be executed. The objective is to make programming simpler by thinking at a higher level, i.e. specifying the potential parallelism of the program without having to handle the adaptation to the effective parallelism. The threads that run the tasks are called worker threads. The allocation is automatically done in runtime using an online scheduling algorithm known as work stealing. Each thread keeps a pool of tasks that are ready to be executed in a deque which is a double-ended queue. Elements can be pushed onto or popped from a deque from both ends. Threads are responsible for task creation, known as task spawning. When a task is spawned by a thread, it is pushed onto the deque of this thread from the top. The thread always pops the task on the top of its deque and executes it. As such, a thread uses its local deque as a stack. If the local deque is empty, the thread tries to pick a task from another randomly chosen thread, called the victim. It pops a task from the bottom of the deque of the victim thread, therefore using the deque of the victim as a queue.

In the case of an application implemented as a dependence graph, tasks are spawned on behalf of the nodes of the graph. When a node receives messages from all its predecessors, a task is spawned on behalf of this node. When run, this task executes the body of the node. When a task finishes its execution it sends a message that is transferred to its predecessors. 

\subsubsection{Implementation}

We used Intel TBB to implement parallel FMI co-simulation in xMOD. The first part which consists in creating the operation graph through the analysis of inter and intra-FMU dependence is the same as in RCOSIM. If the co-simulation is multi-rate, the multi-rate graph transformation is performed as well. Once the operation graph is constructed, an Intel TBB dependence graph which represents this operation graph is automatically created. The graph is of a dependence graph type because we do not manage explicitly data transfer between the different operations since the functions of the FMUs are provided in the form of binaries. Data transfer is implicitly managed by the partial order defined in the operation graph. In other terms, an operation that produces data is necessarily executed before the operation that consumes it. Data writing and reading is done through shared memory and is hidden from the developer. It follows from this that data flow graphs provided by Intel TBB are not suitable for representing such co-simulations because they require explicit management of data transfer between the nodes.

The creation of the dependence graph is done as follows: First, a graph is created and then nodes and edges are added to this graph. A node is created for each operation and added to an array that stores all the created nodes. The first node that is created is a source node which has no predecessor. This node becomes a predecessor of all the nodes that have no predecessor in the operation graph. The body of this node contains the initialization of the co-simulation. Then, for each operation in the operation graph, a function node is created. A function node can have multiple ports to be connected with multiple predecessors and successors. The body of each function node contains the FMU function calls of the corresponding operation. Finally, the edges that connect the nodes are added to the graph. All the created edges are of type continue message. Such edges are used to signal that the execution finished.   

The execution of the co-simulation consists in executing this dependence graph repeatedly, similarly to our offline scheduling approach, i.e. the whole graph is executed at each iteration before a new execution can begin. Initially, one thread is responsible of the creation of the dependence graph and launching the source node. Only the source node, which performs the initialization of the co-simulation, is executed explicitly using a function provided by the Intel TBB library. When this function is called, a task is spawned to execute the body of the source node. Afterward, the runtime library handles the flow of messages in the graph. When the execution of the source node body is finished, it sends a continue message to all its predecessors. Tasks are spawned for the nodes that receive the messages to be executed which in turn send continue messages when their execution is finished and so forth. After all the nodes are executed, the execution is restarted in the same way. The scheduling is managed by the runtime library which creates a pool of working threads and uses the work stealing algorithm described above.

\subsubsection{Comparison}

We implemented a parallelization approach of FMI co-simulations using Intel TBB for the purpose of comparing it with our proposed offline scheduling approach. We have measured the speedups obtained on different numbers of cores using both approaches. First of all, let's summarize the differences between the two approaches. Figure \ref{fig:tbbvsrcosim} illustrates the main steps of both approaches. As stated above, the two first two phases which consist in the construction of the operation graph and the graph transformation in the case of a multi-rate co-simulation are performed in the same way in both approaches. If online scheduling is used, the next step is execution. On the other hand, if offline scheduling is used, two more phases are performed before the execution. The acyclic orientation heuristic is applied on the operation graph to handle mutual exclusion constraints. After this, the offline scheduling heuristic is used to compute a schedule of the operations. During execution, in both the offline and online scheduling approaches, a thread is executed on each core. In the case of offline scheduling, each threads reads the schedule of the corresponding core and executes the operations in the order of this schedule, which does not change during execution. In the case of online scheduling, since no schedule is computed before execution, the runtime library distributes the operations across the the threads during execution in such a way to balance the load. Each thread pushes the operations onto its deque form the top. It executes these operations by popping the operation on the top from its deque, or if its deque is empty, it steals work form another thread by popping an operation for the bottom of this victim thread. Mutual exclusion constraints are handled in online scheduling using lightweight mutex locks provided by the runtime library. These locks have lower cost than mutex locks provided by the OS. 

\begin{figure}[phbt]
\centering
\includestandalone{figures/tbbvsrcosim}
\caption{Comparison of the different phases of the offline and online scheduling approaches.}
\label{fig:tbbvsrcosim}
\end{figure}

We run the co-simulation of the use case on an 8-core Intel core i7 processor running at 2.7 GH with 16GB RAM. The results are shown in table \ref{table:spdup_compr}. The obtained speedup obtained using offline scheduling is better than the one obtained using online scheduling which confirms our assumption. The decision overhead of online scheduling is very costly compared to the execution times of operations which decreases the performance. 

\begin{table}[phbt]
\caption{Comparison of speedup obtained using offline and online scheduling}
\centering
\label{table:spdup_compr}
\begin{tabular}{l c c}
\toprule
\parbox[l]{5cm}{\raggedright Scheduling Approach} & Offline & Online \\
\midrule
Speedup & $2.44$ & $1.64$\\
\bottomrule
\end{tabular}
\end{table}


